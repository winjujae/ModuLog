import os
import torch
import lightning.pytorch as pl  # âœ… ìµœì‹  PyTorch Lightning ì‚¬ìš©
from nemo.collections.asr.models import EncDecDiarLabelModel
from nemo.utils.exp_manager import exp_manager
from omegaconf import OmegaConf
from lightning.pytorch.strategies import DDPStrategy
from lightning.pytorch.callbacks import Callback

# âœ… GPU ì—°ì‚° ìµœì í™” (Tensor Core í™œì„±í™”)
torch.set_float32_matmul_precision("high")

# âœ… ì„¤ì • ë¡œë“œ
ROOT = os.getcwd()
MODEL_CONFIG = os.path.join(ROOT, 'conf', 'msdd_5scl_15_05_50Povl_256x3x32x2.yaml')
config = OmegaConf.load(MODEL_CONFIG)

# âœ… ë°ì´í„° ê²½ë¡œ ì„¤ì •
DATA_DIR = os.path.join(ROOT, 'data', 'kor')
SPLIT_DIR = os.path.join(DATA_DIR, 'splits/w10shift05step50')
MSDD_TRAIN_MANIFEST = os.path.join(SPLIT_DIR, "nemo_manifest_train_10window_50step.json")
MSDD_VALID_MANIFEST = os.path.join(SPLIT_DIR, "nemo_manifest_valid_10window_50step.json")

# âœ… ëª¨ë¸ ì„¤ì • ì¡°ì •
config.model.batch_size = 5
config.trainer.max_epochs = 2
config.model.diarizer.speaker_embeddings.model_path = "titanet_large"

# âœ… ì„ë² ë”© ì €ì¥ í´ë” ì„¤ì •
TRAIN_EMB_DIR = os.path.join(SPLIT_DIR, "nemo_train_time_stamps")
VALID_EMB_DIR = os.path.join(SPLIT_DIR, "nemo_val_time_stamps")

config.model.train_ds.manifest_filepath = MSDD_TRAIN_MANIFEST
config.model.validation_ds.manifest_filepath = MSDD_VALID_MANIFEST
config.model.train_ds.emb_dir = TRAIN_EMB_DIR
config.model.validation_ds.emb_dir = VALID_EMB_DIR

# âœ… ì²´í¬í¬ì¸íŠ¸ ë° ë¡œê·¸ ì €ì¥ í´ë” ì„¤ì •
CHECKPOINT_DIR = os.path.join(ROOT, "checkpoints/w10shift05step50")
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
METRICS_LOG_FILE = os.path.join(CHECKPOINT_DIR, "metrics_log.txt")

# âœ… ê¸°ì¡´ íŒŒì¼ ì‚¬ìš© (speaker embedding model)
config.model.train_ds.emb_dir = os.path.join(SPLIT_DIR, "nemo_train_time_stamps")
config.model.validation_ds.emb_dir = os.path.join(SPLIT_DIR, "nemo_val_time_stamps")

# âœ… ê¸°ì¡´ í´ë” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ì—†ìœ¼ë©´ ìƒì„±í•˜ì§€ ì•ŠìŒ)
if not os.path.exists(config.model.train_ds.emb_dir):
    raise FileNotFoundError(f"âŒ Train embedding directoryê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {config.model.train_ds.emb_dir}")
if not os.path.exists(config.model.validation_ds.emb_dir):
    raise FileNotFoundError(f"âŒ Validation embedding directoryê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {config.model.validation_ds.emb_dir}")

# âœ… exp_manager ì„¤ì • (ì²´í¬í¬ì¸íŠ¸ ìƒì„± ì•ˆ í•¨)
exp_manager_config = config.get("exp_manager", None)
exp_manager_config.create_checkpoint_callback = False  # ğŸš€ ë³€ê²½ëœ ë¶€ë¶„ (ì¤‘ë³µ ë°©ì§€)


# âœ… ì—í­ë³„ ëª¨ë¸ ì €ì¥ ë° ì„±ëŠ¥ ë¡œê¹… ì½œë°±
class SaveModelCallback(Callback):
    def on_validation_epoch_end(self, trainer, pl_module):
        epoch = trainer.current_epoch
        model_path = os.path.join(CHECKPOINT_DIR, f"epoch_{epoch}.nemo")

        # âœ… GPU ìºì‹œ ì •ë¦¬ (OOM ë°©ì§€)
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()

        # âœ… ëª¨ë¸ ì €ì¥ ì‹œ GPU â†’ CPU ì´ë™ (OOM ë°©ì§€)
        pl_module = pl_module.cpu()
        pl_module.save_to(model_path)
        pl_module = pl_module.cuda()

        print(f"ğŸ“Œ Epoch {epoch} ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")

        # âœ… GPU ìºì‹œ ì •ë¦¬ (ë¡œê·¸ ì €ì¥ ì „)
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()

        # âœ… ì„±ëŠ¥ ê¸°ë¡ (val_loss, val_f1_acc)
        if trainer.callback_metrics:
            val_loss = trainer.callback_metrics.get("val_loss", None)
            val_f1_acc = trainer.callback_metrics.get("val_f1_acc", None)
            with open(METRICS_LOG_FILE, "a") as f:
                f.write(f"{epoch}\t{val_loss:.6f}\t{val_f1_acc:.6f}\n")
            print(f"ğŸ“ˆ Epoch {epoch}: val_loss={val_loss:.6f}, val_f1_acc={val_f1_acc:.6f} (ë¡œê·¸ ì €ì¥ ì™„ë£Œ)")
            

# âœ… Trainer ì„¤ì • (3090 Ti 2ê°œ í™œìš© ìµœì í™”)
trainer = pl.Trainer(
    strategy=DDPStrategy(find_unused_parameters=True),
    devices=2,  # 2ê°œ GPU ì‚¬ìš©
    accelerator="gpu",
    max_epochs=config.trainer.max_epochs,  # âœ… ì´ 3 epoch í•™ìŠµ
    #precision="32-true",  # âœ… AMP ì ìš© (ì†ë„ í–¥ìƒ ë° OOM ë°©ì§€)
    callbacks=[SaveModelCallback()],  # âœ… ì½œë°± ì¶”ê°€ (ì—í­ë³„ ëª¨ë¸ ì €ì¥ ë° ì„±ëŠ¥ ë¡œê¹…)
    logger=False,  # âœ… ë¡œê±° ë¹„í™œì„±í™”
    log_every_n_steps=10  # âœ… ë¡œê·¸ ì €ì¥ ë¹ˆë„ ì¡°ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
)

# âœ… exp_manager ì‹¤í–‰ (ìë™ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì•ˆ í•¨)
exp_manager(trainer, exp_manager_config)


# âœ… NeMo ëª¨ë¸ ì´ˆê¸°í™”
msdd_model = EncDecDiarLabelModel(cfg=config.model, trainer=trainer)


msdd_model.setup_training_data(OmegaConf.create({
    "manifest_filepath": MSDD_TRAIN_MANIFEST,
    "emb_dir": TRAIN_EMB_DIR,
    "sample_rate": 16000,
    "soft_label_thres": 0.5,
    "emb_batch_size": 100,
    "batch_size": 5
}))
msdd_model.setup_validation_data(OmegaConf.create({
    "manifest_filepath": MSDD_VALID_MANIFEST,
    "emb_dir": VALID_EMB_DIR,
    "sample_rate": 16000,
    "soft_label_thres": 0.5,
    "emb_batch_size": 100,
    "batch_size": 5
}))


# âœ… ì„±ëŠ¥ ê¸°ë¡ì„ ìœ„í•œ ë¡œê·¸ íŒŒì¼ ìƒì„±
with open(METRICS_LOG_FILE, "w") as f:
    f.write("Epoch\tval_loss\tval_f1_acc\n")

# âœ… í•™ìŠµ ì‹œì‘
trainer.fit(msdd_model)

# âœ… ìµœì¢… ëª¨ë¸ ì €ì¥
MODEL_SAVE_PATH = os.path.join(ROOT, "checkpoints/w10shift05step50", "w10shift05step50_final.nemo")
msdd_model.save_to(MODEL_SAVE_PATH)

print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ìµœì¢… ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {MODEL_SAVE_PATH}")
print(f"ğŸ“Š ì„±ëŠ¥ ë¡œê·¸ íŒŒì¼: {METRICS_LOG_FILE}")
