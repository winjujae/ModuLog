{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "database.yml 형식에 맞게 구성되어있지 않을 경우\n",
    "- 하단 실행\n",
    "- 파일명 변경이니 주의 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # 오디오 파일 이름 맞추기기, 돌릴 때 조심/한번만 돌리기.\n",
    "# for file in os.listdir('Train_Ali_far/audio_dir'):\n",
    "#     file_name = file.split('.')[0]\n",
    "#     new_file_name = file_name.split('_')[0] + \"_\" + file_name.split('_')[1] + \".wav\"\n",
    "#     os.rename('Train_Ali_far/audio_dir/' + file, 'Train_Ali_far/audio_dir/' + new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rttm_dir  = \"Train_Ali_far/rttm\"\n",
    "# train_dir = \"Train_Ali_far/rttm/train/\"\n",
    "# test_dir = \"Train_Ali_far/rttm/test/\"\n",
    "# dev_dir = \"Train_Ali_far/rttm/dev/\"\n",
    "\n",
    "# os.makedirs(train_dir, exist_ok=True)\n",
    "# os.makedirs(test_dir, exist_ok=True)\n",
    "# os.makedirs(dev_dir, exist_ok=True)\n",
    "\n",
    "# tg_files = [f for f in os.listdir(rttm_dir) if f.endswith('.rttm')]\n",
    "\n",
    "# # train, test, dev split\n",
    "# train_count = int(len(tg_files) * 0.8)\n",
    "# test_count = int(len(tg_files) * 0.1)\n",
    "# dev_count = len(tg_files) - train_count - test_count\n",
    "\n",
    "# train_files = tg_files[:train_count]\n",
    "# test_files = tg_files[train_count:train_count + test_count]\n",
    "# dev_files = tg_files[train_count + test_count:]\n",
    "\n",
    "\n",
    "# with open(\"Train_Ali_far/train_list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for i, file in enumerate(train_files):\n",
    "#         f.write(file.split('.')[0] + \"\\n\")\n",
    "\n",
    "\n",
    "# # test list txt file\n",
    "# with open(\"Train_Ali_far/test_list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for i, file in enumerate(test_files):\n",
    "#         f.write(file.split('.')[0] + \"\\n\")\n",
    "\n",
    "# # dev list txt file\n",
    "# with open(\"Train_Ali_far/dev_list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for i, file in enumerate(dev_files):\n",
    "#         f.write(file.split('.')[0] + \"\\n\")\n",
    "\n",
    "# # move files\n",
    "# for f in train_files:\n",
    "#     os.rename(os.path.join(rttm_dir, f), os.path.join(train_dir, f))\n",
    "\n",
    "# for f in test_files:\n",
    "#     os.rename(os.path.join(rttm_dir, f), os.path.join(test_dir, f))\n",
    "\n",
    "# for f in dev_files:\n",
    "#     os.rename(os.path.join(rttm_dir, f), os.path.join(dev_dir, f))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydub import AudioSegment\n",
    "# import os\n",
    "\n",
    "# os.mkdir('Train_Ali_far/uem')\n",
    "# os.mkdir('Train_Ali_far/uem/train')\n",
    "# os.mkdir('Train_Ali_far/uem/test')\n",
    "# os.mkdir('Train_Ali_far/uem/dev')\n",
    "\n",
    "# def get_duration(file):\n",
    "#     audio = AudioSegment.from_wav(file)\n",
    "#     return audio.duration_seconds\n",
    "\n",
    "# #get train list file name\n",
    "# train_list = []\n",
    "# with open('Train_Ali_far/train_list.txt', 'r', encoding='utf-8') as f:\n",
    "#     train_list = f.readlines()\n",
    "# train_list = [x.strip() for x in train_list]\n",
    "\n",
    "# #get test list file name\n",
    "# test_list = []\n",
    "# with open('Train_Ali_far/test_list.txt', 'r', encoding='utf-8') as f:\n",
    "#     test_list = f.readlines()\n",
    "# test_list = [x.strip() for x in test_list]\n",
    "\n",
    "# #get dev list file name\n",
    "# dev_list = []\n",
    "# with open('Train_Ali_far/dev_list.txt', 'r', encoding='utf-8') as f:\n",
    "#     dev_list = f.readlines()\n",
    "# dev_list = [x.strip() for x in dev_list]\n",
    "\n",
    "\n",
    "# for file in os.listdir('Train_Ali_far/audio_dir'):\n",
    "#     file_name = file.split('.')[0]\n",
    "#     duration = get_duration('Train_Ali_far/audio_dir/' + file) \n",
    "\n",
    "#     if file_name in train_list:\n",
    "#         #generate uem file\n",
    "#         with open('Train_Ali_far/uem/train/' + file_name + '.uem', 'w', encoding='utf-8') as f:\n",
    "#             f.write(file_name + ' 1 ' + '0.00 ' + str(duration))\n",
    "    \n",
    "#     elif file_name in test_list:\n",
    "#         #generate uem file\n",
    "#         with open('Train_Ali_far/uem/test/' + file_name + '.uem', 'w', encoding='utf-8') as f:\n",
    "#             f.write(file_name + ' 1 ' + '0.00 ' + str(duration))\n",
    "    \n",
    "#     elif file_name in dev_list:\n",
    "#         #generate uem file\n",
    "#         with open('Train_Ali_far/uem/dev/' + file_name + '.uem', 'w', encoding='utf-8') as f:\n",
    "#             f.write(file_name + ' 1 ' + '0.00 ' + str(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파인튜닝 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.database import registry, FileFinder\n",
    "\n",
    "registry.load_database(\"/modu/docs/fine_tuning/database.yml\")\n",
    "dataset = registry.get_protocol(\"ALI.SpeakerDiarization.jojo\", {\"audio\": FileFinder()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYANNOTE_DATABASE_CONFIG=\"database.yml\" pyannote-database info ALI.SpeakerDiarization.jojo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the base segmentation model for fine-tuning\n",
    "from pyannote.audio import Pipeline\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "pretrained_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\")) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrained_pipeline = pretrained_pipeline.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "\n",
    "metric = DiarizationErrorRate()\n",
    "\n",
    "for file in dataset.test():\n",
    "    # apply pretrained pipeline\n",
    "    file[\"pretrained pipeline\"] = pretrained_pipeline(file)\n",
    "\n",
    "    # evaluate its performance\n",
    "    metric(file[\"annotation\"], file[\"pretrained pipeline\"], uem=file[\"annotated\"])\n",
    "\n",
    "print(f\"The pretrained pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric):.1f}% on {dataset.name} test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file['annotation'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file['pretrained pipeline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning the segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Model\n",
    "import os\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "model = Model.from_pretrained(\"pyannote/segmentation-3.0\", use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "from pyannote.audio import Model\n",
    "import os\n",
    "import torch\n",
    "\n",
    "model = Model.from_pretrained(\"epoch=0.ckpt\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.tasks import Segmentation\n",
    "task = Segmentation(\n",
    "    dataset, \n",
    "    duration=model.specifications.duration, \n",
    "    max_num_speakers=len(model.specifications.classes), \n",
    "    batch_size=64,\n",
    "    num_workers=4, \n",
    "    loss=\"bce\", \n",
    "    vad_loss=\"bce\")\n",
    "model.task = task\n",
    "model.prepare_data()\n",
    "model.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes approximately 15min to run on Google Colab GPU\n",
    "from types import MethodType\n",
    "from torch.optim import Adam\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    RichProgressBar,\n",
    ")\n",
    "\n",
    "# we use Adam optimizer with 1e-4 learning rate\n",
    "def configure_optimizers(self):\n",
    "    return Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "model.configure_optimizers = MethodType(configure_optimizers, model)\n",
    "\n",
    "# we monitor diarization error rate on the validation set\n",
    "# and use to keep the best checkpoint and stop early\n",
    "monitor, direction = task.val_monitor\n",
    "checkpoint = ModelCheckpoint(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    save_top_k=1,\n",
    "    every_n_epochs=1,\n",
    "    save_last=False,\n",
    "    save_weights_only=False,\n",
    "    filename=\"{epoch}\",\n",
    "    verbose=False,\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    min_delta=0.0,\n",
    "    patience=10,\n",
    "    strict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "callbacks = [RichProgressBar(), checkpoint, early_stopping]\n",
    "\n",
    "# we train for at most 20 epochs (might be shorter in case of early stopping)\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(accelerator=\"gpu\", \n",
    "                  callbacks=callbacks, \n",
    "                  max_epochs=20,\n",
    "                  gradient_clip_val=0.5)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = checkpoint.best_model_path\n",
    "\n",
    "#to file\n",
    "import shutil\n",
    "shutil.move(finetuned_model, \"finetuned_model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 로딩 시 실행.\n",
    "# we load the best model\n",
    "from pyannote.audio import Model\n",
    "import torch\n",
    "\n",
    "finetuned_model = \"epoch=0.ckpt\"\n",
    "model = Model.from_pretrained(finetuned_model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the pipeline hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_hyperparameters = pretrained_pipeline.parameters(instantiated=True)\n",
    "pretrained_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes approximately 5min to run on Google Colab GPU\n",
    "from pyannote.audio.pipelines import SpeakerDiarization\n",
    "from pyannote.pipeline import Optimizer\n",
    "\n",
    "pipeline = SpeakerDiarization(\n",
    "    segmentation=finetuned_model,\n",
    "    clustering=\"OracleClustering\",\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipeline = pipeline.to(device)\n",
    "\n",
    "# as reported in the technical report, min_duration_off can safely be set to 0.0\n",
    "pipeline.freeze({\"segmentation\": {\"min_duration_off\": 0.0}})\n",
    "\n",
    "optimizer = Optimizer(pipeline)\n",
    "dev_set = list(dataset.development())\n",
    "\n",
    "iterations = optimizer.tune_iter(dev_set, show_progress=True)\n",
    "best_loss = 1.0\n",
    "for i, iteration in enumerate(iterations):\n",
    "    print(f\"Iteration {i}\")\n",
    "    print(f\"Current segmentation threshold: {iteration['params']['segmentation']['threshold']}\")\n",
    "    print(f\"Current segmentation params: {iteration['params']['segmentation']}\")  # segmentation 파라미터 출력\n",
    "    if i > 20: break  # 50 iterations should give slightly better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_segmentation_threshold = optimizer.best_params[\"segmentation\"][\"threshold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this takes approximately 5min to run on Google Colab GPU\n",
    "pipeline = SpeakerDiarization(\n",
    "    segmentation=finetuned_model,\n",
    "    embedding=pretrained_pipeline.embedding,\n",
    "    embedding_exclude_overlap=pretrained_pipeline.embedding_exclude_overlap,\n",
    "    clustering=pretrained_pipeline.klustering,\n",
    ")\n",
    "\n",
    "pipeline.freeze({\n",
    "    \"segmentation\": {\n",
    "        \"threshold\": best_segmentation_threshold,\n",
    "        \"min_duration_off\": 0.0,\n",
    "    },\n",
    "    \"clustering\": {\n",
    "        \"method\": \"centroid\",\n",
    "        \"min_cluster_size\": 15,\n",
    "    },\n",
    "})\n",
    "\n",
    "optimizer = Optimizer(pipeline)\n",
    "iterations = optimizer.tune_iter(dev_set, show_progress=False)\n",
    "best_loss = 1.0\n",
    "for i, iteration in enumerate(iterations):\n",
    "    print(f\"Best clustering threshold so far: {iteration['params']['clustering']['threshold']}\")\n",
    "    if i > 20: break  # 50 iterations should give slightly better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clustering_threshold = optimizer.best_params['clustering']['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes approximately 2min to run on Google Colab GPU\n",
    "finetuned_pipeline = SpeakerDiarization(\n",
    "    segmentation=finetuned_model,\n",
    "    embedding=pretrained_pipeline.embedding,\n",
    "    embedding_exclude_overlap=pretrained_pipeline.embedding_exclude_overlap,\n",
    "    clustering=pretrained_pipeline.klustering,\n",
    ")\n",
    "\n",
    "finetuned_pipeline.instantiate({\n",
    "    \"segmentation\": {\n",
    "        \"threshold\": best_segmentation_threshold,\n",
    "        \"min_duration_off\": 0.0,\n",
    "    },\n",
    "    \"clustering\": {\n",
    "        \"method\": \"centroid\",\n",
    "        \"min_cluster_size\": 15,\n",
    "        \"threshold\": best_clustering_threshold,\n",
    "    },\n",
    "})\n",
    "\n",
    "metric = DiarizationErrorRate()\n",
    "\n",
    "for file in dataset.test():\n",
    "    # apply finetuned pipeline\n",
    "    file[\"finetuned pipeline\"] = finetuned_pipeline(file)\n",
    "\n",
    "    # evaluate its performance\n",
    "    metric(file[\"annotation\"], file[\"finetuned pipeline\"], uem=file[\"annotated\"])\n",
    "\n",
    "print(f\"The finetuned pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric):.1f}% on {dataset.name} test set.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
