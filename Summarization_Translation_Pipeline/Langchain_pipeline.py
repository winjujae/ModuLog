import os
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

class RAGpipeline:
    def __init__(self, pdf_path: str, chunk_size=1000, chunk_overlap=50, k=10, max_tokens=512):
        """
        RAG íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        - pdf_path: ì²˜ë¦¬í•  PDF ë¬¸ì„œ ê²½ë¡œ
        - chunk_size: ë¬¸ì„œ ë¶„í•  í¬ê¸°
        - chunk_overlap: ë¬¸ì„œ ë¶„í•  ì¤‘ ê²¹ì¹˜ëŠ” ë¶€ë¶„ í¬ê¸°
        - k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜
        - max_tokens: LLM ì‘ë‹µ ìµœëŒ€ í† í° ìˆ˜
        """
        self.pdf_path = pdf_path
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.k = k
        self.max_tokens = max_tokens
        self.vectorstore = None
        self.retriever = None
        self.rag_chain = None
        
        self._load_and_process_pdf()
        self._setup_retriever()
        self._setup_rag_pipeline()

    @classmethod
    def from_text(cls, text: str, chunk_size=1000, chunk_overlap=50, k=10, max_tokens=512):
        """í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥ë°›ì•„ RAG íŒŒì´í”„ë¼ì¸ì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ"""
        instance = cls.__new__(cls)  # ë¹ˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        instance.pdf_path = None
        instance.chunk_size = chunk_size
        instance.chunk_overlap = chunk_overlap
        instance.k = k
        instance.max_tokens = max_tokens

        # ë¬¸ì„œ â†’ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        documents = [Document(page_content=text)]

        # í…ìŠ¤íŠ¸ ë¶„í• 
        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        chunks = splitter.split_documents(documents)

        # ì„ë² ë”©
        embeddings = OpenAIEmbeddings(model="text-embedding-ada-002", openai_api_key=OPENAI_API_KEY)
        instance.vectorstore = FAISS.from_documents(chunks, embeddings)

        # ê²€ìƒ‰ê¸° ë° RAG êµ¬ì„±
        instance._setup_retriever()
        instance._setup_rag_pipeline()
        return instance
    
    def _load_and_process_pdf(self):
        """PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œë¥¼ êµ¬ì¶•í•œë‹¤."""
        # ë¬¸ì„œ ë¡œë“œ
        loader = PyMuPDFLoader(self.pdf_path)
        documents = loader.load()
        
        # ë¬¸ì„œ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap
        )
        chunks = text_splitter.split_documents(documents)

        # ì„ë² ë”© ìƒì„±
        embeddings = OpenAIEmbeddings(model="text-embedding-ada-002", openai_api_key=OPENAI_API_KEY)
        self.vectorstore = FAISS.from_documents(chunks, embeddings)

    def _setup_retriever(self):
        # FAISS ê¸°ë°˜ ê²€ìƒ‰ê¸° ì„¤ì •
        self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": self.k})

    def _setup_rag_pipeline(self):
        # RAG íŒŒì´í”„ë¼ì¸ êµ¬ì„±
        prompt_template = PromptTemplate(
            input_variables=["context", "question"],
            template="""
            ë‹¹ì‹ ì€ PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AIì…ë‹ˆë‹¤.
            ë§Œì•½ ë‹µë³€ì„ ëª¨ë¥¸ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ì–˜ê¸°í•˜ë©´ ë©ë‹ˆë‹¤.

            ë¬¸ì„œ ë‚´ìš©:
            {context}

            ì‚¬ìš©ì ì§ˆë¬¸:
            {question}

            ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ìµœëŒ€í•œ ìƒì„¸í•˜ê³  ê´€ë ¨ì„±ì´ ë†’ì€ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
            """
        )

        llm = ChatOpenAI(
            model_name="gpt-4o",
            temperature=0,
            openai_api_key=OPENAI_API_KEY,
            max_tokens=self.max_tokens
        )

        self.rag_chain = (
            {"context": self.retriever, "question": RunnablePassthrough()}
            | prompt_template
            | llm
            | StrOutputParser()
        )

    def ask(self, query: str):
        # RAG ì‹œìŠ¤í…œ ì‚¬ìš©ì ë‹µë³€ ìƒì„±
        response = self.rag_chain.invoke(query)
        return response

def run_rag_pipeline(pdf_path=None, input_text=None, query=None):
    if pdf_path:
        print("ğŸ“„ PDF ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
        rag_system = RAGpipeline(pdf_path)
    else:
        print("ğŸ“ í…ìŠ¤íŠ¸ ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
        if input_text is None:
            input_text = """
            ì–´ë– í•œ íšŒì˜ë„ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
            íšŒì˜ë¡ì„ ë‹¤ì‹œ í•œë²ˆ ê²€í† í•´ì£¼ì„¸ìš”!
            """
        rag_system = RAGpipeline.from_text(input_text)

    # query ì§ì ‘ ì…ë ¥ë°›ê¸°
    if query is None:
        print("\nâ“ ìš”ì•½ì´ë‚˜ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” (ì˜ˆ: ì˜¤ëŠ˜ íšŒì˜ í•µì‹¬ ì•Œë ¤ì¤˜):")
        query = input("ğŸ—£ï¸ ì‚¬ìš©ì ì§ˆë¬¸: ").strip()
        if not query:
            print("âš ï¸ ì§ˆë¬¸ì´ ì…ë ¥ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            query = "ì˜¤ëŠ˜ íšŒì˜ëŠ” í•µì‹¬ì´ ë­ì•¼?"

    response = rag_system.ask(query)
    print("\nğŸ’¡ RAG ì‘ë‹µ:")
    print(response)